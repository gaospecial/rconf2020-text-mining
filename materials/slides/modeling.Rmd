---
title: "Text Modeling"
subtitle: "<br><br>USING TIDY DATA PRINCIPLES"
author: "Julia Silge | rstudio::conf | 28 Jan 2020"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css", "css/footer_plus.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        width = 80)
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, dpi = 300)
library(ggplot2)
library(silgelib)
theme_set(theme_roboto())
```

layout: true

<div class="my-footer"><span>bit.ly/silge-rstudioconf-2</span></div> 

---

class: inverse, center, bottom

background-image: url(figs/robert-bye-R-WtV-QyVnY-unsplash.jpg)
background-size: cover


# WELCOME!

### Text Mining Using Tidy Data Principles

---

class: inverse, center, middle

background-image: url(figs/p_and_p_cover.png)
background-size: cover


# Text Modeling

<img src="figs/blue_jane.png" width="150px"/>

### USING TIDY PRINCIPLES

.large[Julia Silge | rstudio::conf | 28 Jan 2020]

---

class: middle, center

.pull-left[
# <i class="fa fa-wifi"></i>

Wifi network name  

.large[rstudio20]

]

.pull-left[
# <i class="fa fa-key"></i>

Wifi password

.large[tidyverse20]

]

---

<img src="figs/blue_jane.png" style="position:absolute;top:30px;right:30px;" width="100px"/>

## **Workshop policies**

--

- .large[Identify the exits closest to you in case of emergency] 

--

- .large[Please review the rstudio::conf code of conduct that applies to all workshops]

--

- .large[CoC issues can be addressed three ways:]

  - In person: contact any rstudio::conf staff member or the conference registration desk
  - By email: send a message to `conf@rstudio.com`
  - By phone: call 844-448-1212

--

- .large[Please do not photograph people wearing red lanyards]

--

- .large[A chill-out room is available for neurologically diverse attendees on the 4th floor of tower 1]

---

class: right, middle

<img src="figs/blue_jane.png" width="150px"/>

# Find me at...

<a href="http://twitter.com/juliasilge"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

---

class: left, top

<img src="figs/blue_jane.png" width="150px"/>

# Meet your TAs

## `r emo::ji("dizzy")` Emil Hvitfelt (coordinator)

## `r emo::ji("boom")` Jeroen Claes

## `r emo::ji("sparkles")` Kasia Kulma

---

class: left, top

<img src="figs/blue_jane.png" width="150px"/>

# Asking for help

--

## `r emo::ji("sos")`  "I'm stuck"

--

## `r emo::ji("warning")`  "I'm not stuck, but I need help on my computer"

--

## `r emo::ji("raising_hand_woman")`  "I need help understanding something"

---

class: right, inverse, middle

background-image: url(figs/p_and_p_cover.png)
background-size: cover

# TIDYING AND CASTING 

<h1 class="fa fa-check-circle fa-fw"></h1>

---

background-image: url(figs/tmwr_0601.png)
background-size: 900px

---

class: inverse

background-image: url(figs/p_and_p_cover.png)
background-size: cover

# Two powerful NLP techniques

--

### `r emo::ji("bulb")` Topic modeling

--

### `r emo::ji("bulb")` Text classification

---

## Let's install some packages

```{r, eval=FALSE}
install.packages(c("tidyverse", 
                   "tidytext",
                   "gutenbergr",                   
                   "tidymodels",
                   "stm",
                   "glmnet"))
```


---

class: inverse

background-image: url(figs/p_and_p_cover.png)
background-size: cover

# Topic modeling

### `r emo::ji("open_book")` Each DOCUMENT = mixture of topics

--

### `r emo::ji("bookmark_tabs")` Each TOPIC = mixture of tokens

---

class: top

background-image: url(figs/top_tags-1.png)
background-size: 800px

---

class: center, middle, inverse

background-image: url(figs/p_and_p_cover.png)
background-size: cover

# GREAT LIBRARY HEIST `r emo::ji("sleuth")`

---

## **Downloading your text data**

```{r}
library(tidyverse)
library(gutenbergr)

books <- gutenberg_download(c(164, 36, 1342, 1400),
                            meta_fields = "title")

books %>%
  count(title)
```

---

## **Someone has torn your books apart!** `r emo::ji("sob")`

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r eval = FALSE}
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, 
                                     regex("^chapter ", 
                                           ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

glimpse(by_chapter)
```

---

## **Someone has torn your books apart!** `r emo::ji("sob")`

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r}
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, 
                                     regex("^chapter ", 
                                           ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

glimpse(by_chapter)
```

---

## **Can we put them back together?**

```{r}
library(tidytext)

word_counts <- by_chapter %>%
  unnest_tokens(word, text) %>%               #<<
  anti_join(get_stopwords()) %>%
  count(document, word, sort = TRUE)

glimpse(word_counts)

```

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[The dataset `word_counts` contains]

- .large[the counts of words per book]
- .large[the counts of words per chapter]
- .large[the counts of words per line]

---

## **Can we put them back together?**

```{r}
words_sparse <- word_counts %>%
  cast_sparse(document, word, n)         #<<

class(words_sparse)

dim(words_sparse)
```

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[Is `words_sparse` a tidy dataset?]

- .large[Yes `r emo::ji("check")`]
- .large[No `r emo::ji("no_entry_sign")`]

---

## **Train a topic model**

Use a sparse matrix or a `quanteda::dfm` object as input

```{r}
library(stm)

topic_model <- stm(words_sparse, K = 4, 
                   verbose = FALSE, 
                   init.type = "Spectral")

```
---

## **Train a topic model**

Use a sparse matrix or a `quanteda::dfm` object as input

```{r}
summary(topic_model)
```


---

## **Exploring the output of topic modeling**


```{r}
chapter_topics <- tidy(topic_model, matrix = "beta")

chapter_topics
```

---

## **Exploring the output of topic modeling**

.unscramble[U N S C R A M B L E]

```
top_terms <- chapter_topics %>%
```
```
ungroup() %>%
```
```
group_by(topic) %>%
```
```
arrange(topic, -beta)
```
```
top_n(10, beta) %>%
```


---

## **Exploring the output of topic modeling**

```{r}
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

---

## **Exploring the output of topic modeling**

```{r}
top_terms
```

---
## **Exploring the output of topic modeling**

```{r, eval=FALSE}
top_terms %>%
  mutate(term = fct_reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +         #<<
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

---

```{r, echo=FALSE, fig.height=3.9}
top_terms %>%
  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = expression(beta), x = NULL)
```

---

## **How are documents classified?**

```{r}
chapters_gamma <- tidy(topic_model, matrix = "gamma",
                       document_names = rownames(words_sparse))

chapters_gamma
```

---

## **How are documents classified?**

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r eval=FALSE}
chapters_parsed <- chapters_gamma %>%
  separate(document, c("title", "chapter"), 
           sep = "_", convert = TRUE)

chapters_parsed
```

---

## **How are documents classified?**

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r}
chapters_parsed <- chapters_gamma %>%
  separate(document, c("title", "chapter"), 
           sep = "_", convert = TRUE)

glimpse(chapters_parsed)
```

---

## **How are documents classified?**

.unscramble[U N S C R A M B L E]

```
chapters_parsed %>%
```
```
ggplot(aes(factor(topic), gamma)) +
```
```
facet_wrap(~ title)
```
```
mutate(title = fct_reorder(title, gamma * topic)) %>%
```
```
geom_boxplot() +
```

---

## **How are documents classified?**

```{r, eval=FALSE}
chapters_parsed %>%
  mutate(title = fct_reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
```

---

```{r, echo=FALSE, fig.height=3.9}
chapters_parsed %>%
  mutate(title = fct_reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma, color = factor(topic))) +
  geom_boxplot(show.legend = FALSE) +
  facet_wrap(~ title) +
  labs(x = "Topic", y = expression(gamma))
```

---

class: center, middle, inverse

background-image: url(figs/p_and_p_cover.png)
background-size: cover

# GOING FARTHER `r emo::ji("rocket")`

---

## Tidying model output

### Which words in each document are assigned to which topics?

- .large[`augment()`]
- .large[Add information to each observation in the original data]

---

background-image: url(figs/stm_video.png)
background-size: 850px

---

## **Using stm**

- .large[Document-level covariates]

```{r, eval=FALSE}
topic_model <- stm(words_sparse, 
                   K = 0, init.type = "Spectral",
                   prevalence = ~s(Year),
                   data = covariates,
                   verbose = FALSE)
```

- .large[Use functions for `semanticCoherence()`, `checkResiduals()`, `exclusivity()`, and more!]

- .large[Check out http://www.structuraltopicmodel.com/]

- .large[See [my blog post](https://juliasilge.com/blog/evaluating-stm/) for how to choose `K`, the number of topics]

---


background-image: url(figs/model_diagnostic-1.png)
background-position: 50% 50%
background-size: 950px

---

# Stemming?

.large[Advice from [Schofield & Mimno](https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf)]

.large["Comparing Apples to Apple: The Effects of Stemmers on Topic Models"]

---

class: right, middle

<h1 class="fa fa-quote-left fa-fw"></h1>

<h2> Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability. </h2>

<h1 class="fa fa-quote-right fa-fw"></h1>
---

## **Train many topic models**

```{r}
library(furrr)
plan(multicore)
plan(multisession)

many_models <- tibble(K = c(3, 4, 6, 8, 10)) %>%        #<<
  mutate(topic_model = future_map(K, 
                                  ~stm(words_sparse, K = .,
                                       verbose = FALSE)))

many_models
```

---

## **Train many topic models**

```{r}
heldout <- make.heldout(words_sparse)

k_result <- many_models %>%
  mutate(exclusivity        = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, words_sparse),
         eval_heldout       = map(topic_model, eval.heldout, heldout$missing),
         residual           = map(topic_model, checkResiduals, words_sparse),
         bound              = map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact              = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound             = bound + lfact,
         iterations         = map_dbl(topic_model, function(x) length(x$convergence$bound)))
```

---

## **Train many topic models**

```{r}
k_result
```

---

## **Train many topic models**

```{r, eval=FALSE}
k_result %>%
  transmute(K,
            `Lower bound`         = lbound,
            Residuals             = map_dbl(residual, "dispersion"),       #<<
            `Semantic coherence`  = map_dbl(semantic_coherence, mean),       #<<
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%       #<<
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line() +
  facet_wrap(~Metric, scales = "free_y")
```

---

```{r, echo=FALSE, fig.height=3.9}
k_result %>%
  transmute(K,
            `Lower bound`         = lbound,
            Residuals             = map_dbl(residual, "dispersion"),
            `Semantic coherence`  = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL)
```

---

## **What is semantic coherence?**

- .large[Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together]

--

- .large[Correlates well with human judgment of topic quality `r emo::ji("happy")`]

--

- .large[Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words `r emo::ji("sad")`]

--

- .large[Measure semantic coherence **and** exclusivity]

---

## **Train many topic models**

```{r, eval=FALSE}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(3, 6, 10)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  ggplot(aes(semantic_coherence, exclusivity, 
             color = factor(K))) +
  geom_point()
```

---

```{r, echo=FALSE, fig.height=3.9}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(3, 6, 10)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, 
             color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity")
```

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[Topic modeling is an example of...]

- .unscramble[supervised machine learning]
- .unscramble[unsupervised machine learning]


---

class: right, middle, inverse

background-image: url(figs/p_and_p_cover.png)
background-size: cover


# TEXT CLASSIFICATION
<h1 class="fa fa-balance-scale fa-fw"></h1>

---

## **Downloading your text data**

```{r}
library(tidyverse)
library(gutenbergr)

titles <- c("The War of the Worlds",
            "Pride and Prejudice")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title") %>%
  mutate(document = row_number())

glimpse(books)
```

---

## **Making a tidy dataset**

.large[Use this kind of data structure for EDA! `r emo::ji("nail")`]

```{r}
library(tidytext)

tidy_books <- books %>%
  unnest_tokens(word, text) %>%           #<<
  group_by(word) %>%
  filter(n() > 10) %>%
  ungroup

glimpse(tidy_books)
```

---

## **Create training and testing sets**

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r}
library(rsample)

books_split <- tidy_books %>%
  distinct(document) %>%
  initial_split()           #<<

train_data <- training(books_split)
test_data <- testing(books_split)
```


---

## **Cast to a sparse matrix**

```{r}
sparse_words <- tidy_books %>%
  count(document, word, sort = TRUE) %>%
  inner_join(train_data) %>%
  cast_sparse(document, word, n)               #<<

class(sparse_words)

dim(sparse_words)
```

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[Which `dim` of the sparse matrix is the number of features?]

- .large[`r dim(sparse_words)[1]`]
- .large[`r dim(sparse_words)[2]`]

.large[Feature = term = word]

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[If you want to use tf-idf instead of counts, should you calculate tf-idf before or after splitting train and test?]

- .large[Before]
- .large[After]

---

## **Build a dataframe with the response variable**


```{r}
word_rownames <- as.integer(rownames(sparse_words))

books_joined <- tibble(document = word_rownames) %>%
  left_join(books %>%
              select(document, title))

glimpse(books_joined)
```


---

## **Train a glmnet model**

```{r}
library(glmnet)
library(doMC)
registerDoMC(cores = 8)

is_jane <- books_joined$title == "Pride and Prejudice"

model <- cv.glmnet(sparse_words, is_jane, 
                   family = "binomial", 
                   parallel = TRUE, 
                   keep = TRUE)

```

- .large[Regularization constrains magnitude of coefficients]

- .large[LASSO performs feature selection]


---

## **Tidying our model**

.large[Tidy, then filter to choose some lambda from glmnet output]

```{r}
library(broom)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)

Intercept <- coefs %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
```

---

## **Tidying our model**

.unscramble[U N S C R A M B L E]

```
classifications <- tidy_books %>%
```
```
mutate(probability = plogis(Intercept + score))
```
```
inner_join(test_data) %>%
```
```
group_by(document) %>%
```
```
inner_join(coefs, by = c("word" = "term")) %>%
```
```
summarize(score = sum(estimate)) %>%
```

---

## **Tidying our model**

```{r}
classifications <- tidy_books %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(document) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(Intercept + score))

glimpse(classifications)
```

---

## **Understanding our model**

.unscramble[U N S C R A M B L E]

```
coefs %>%
```
```
group_by(estimate > 0) %>%
```
```
coord_flip()
```
```
geom_col(show.legend = FALSE) +
```
```
ungroup %>%
```
```
top_n(10, abs(estimate)) %>%
```
```
ggplot(aes(fct_reorder(term, estimate), 
estimate, 
fill = estimate > 0)) +
```

---

## **Understanding our model**

```{r, eval=FALSE}
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup %>%
  ggplot(aes(fct_reorder(term, estimate), 
             estimate, 
             fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip()
```


---

```{r, echo = FALSE, fig.height=3.9}
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL,
       title = "Coefficients that increase/decrease probability",
       subtitle = "A document mentioning Martians is unlikely to be written by Jane Austen")
```

---

## **ROC**

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r eval=FALSE}
comment_classes <- classifications %>%
  left_join(books %>%
              select(title, document), by = "document") %>%
  mutate(title = as.factor(title))

comment_classes
```

---

## **ROC**

.large[What do you predict will happen if we run the following code? `r emo::ji("thinking")`]

```{r}
comment_classes <- classifications %>%
  left_join(books %>%
              select(title, document), by = "document") %>%
  mutate(title = as.factor(title))

glimpse(comment_classes)
```

---

## **ROC**

```{r eval=FALSE}
library(yardstick)

comment_classes %>%
  roc_curve(title, probability) %>%              #<<
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```

---

```{r, echo = FALSE, fig.height=3.9}
library(yardstick)

comment_classes %>%
  roc_curve(title, probability) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(
    color = "midnightblue",
    size = 1.5
  ) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  labs(
    title = "ROC curve for text classification"
  )
```

---

## **AUC for model**

```{r}
comment_classes %>%
  roc_auc(title, probability)
```

---

<img src="figs/blue_jane.png" width="150px"/>

## Jane wants to know...

.large[Is this the AUC for the training or testing data?]

- .large[Training]
- .large[Testing]

---

## **Confusion matrix**

```{r}
comment_classes %>%
  mutate(
    prediction = case_when(
      probability > 0.5 ~ "Pride and Prejudice",
      TRUE ~ "The War of the Worlds"
    ),
    prediction = as.factor(prediction)
  ) %>%
  conf_mat(title, prediction)             #<<
```

---

## **Misclassifications**

Let's talk about misclassifications. Which documents here were incorrectly predicted to be written by Jane Austen?

```{r}
comment_classes %>%
  filter(probability > .8, title == "The War of the Worlds") %>%       #<<
  sample_n(5) %>%
  inner_join(books %>%
               select(document, text)) %>%
  select(probability, text)
```

---

## **Misclassifications**

Let's talk about misclassifications. Which documents here were incorrectly predicted to *not* be written by Jane Austen?

```{r}
comment_classes %>%
  filter(probability < .3, title == "Pride and Prejudice" ) %>%      #<<
  sample_n(5) %>%
  inner_join(books %>%
               select(document, text)) %>%
  select(probability, text)
```

---

background-image: url(figs/tmwr_0601.png)
background-position: 50% 70%
background-size: 750px

## **Workflow for text mining/modeling**

---

background-image: url(figs/lizzieskipping.gif)
background-position: 50% 55%
background-size: 750px

# **Go explore real-world text!**

---

class: left, middle

<img src="figs/blue_jane.png" width="150px"/>

# Thanks!

<a href="http://twitter.com/juliasilge"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

Slides created with [**remark.js**](http://remarkjs.com/) and the R package [**xaringan**](https://github.com/yihui/xaringan)

---

class: middle, center

# <i class="fa fa-check-circle"></i>

# Submit feedback before you leave

.large[[rstd.io/ws-survey](https://rstd.io/ws-survey)]

