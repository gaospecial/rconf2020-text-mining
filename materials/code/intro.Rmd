---
title: "Text mining -- rstudio::conf 2020"
date: "2020/01/27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Access the full text of one book

What book do *you* want to analyze today?

Replace `1342` below with your own choice:
https://www.gutenberg.org/browse/scores/top

```{r}
library(tidyverse)
library(tidytext)
library(gutenbergr)

full_text <- gutenberg_download(1342)
```

Now it's time to tokenize and tidy this text data.

```{r}
tidy_book <- full_text %>%
  mutate(line = row_number()) %>%
  unnest_tokens(word, text)

tidy_book
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
tidy_book %>%
  count(word, sort = TRUE)
```

## Stop words


```{r}
get_stopwords()
```

Try out some

- different languages (`language`)
- different sources (`source`)

```{r eval=FALSE}
# 不能获取中文的停用词
get_stopwords(language = "zh")
```


## What are the most common words?

**U N S C R A M B L E**

```{r}
tidy_book %>%
  anti_join(get_stopwords(source = "smart")) %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(fct_reorder(word, n), n)) +  
  geom_col() +
  coord_flip()
```


## Sentiment lexicons（情绪词汇）

Explore some sentiment lexicons.

```{r}
get_sentiments("bing")
```

Implement sentiment analysis with an `inner_join()`

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment, sort = TRUE)
```


What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%          count(word, sentiment, sort = TRUE) 
```

What words contribute the most to sentiment scores for **your** book?

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment, word, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(fct_reorder(word, n),
             n, 
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") 
```


## Term frequency and inverse document frequency

Go back to Project Gutenberg and make a collection (*corpus*) for yourself!

```{r}
full_collection <- gutenberg_download(c(1342, 158, 161, 141),
                                      meta_fields = "title")

full_collection
```

Count word frequencies in your collection.

```{r}
book_words <- full_collection %>%
  unnest_tokens(word,text ),
  count(title, word, sort = TRUE)

book_words
```

Calculate tf-idf.

```{r}
book_tfidf <- book_words %>%
  bind_tf_idf(word, title, n)

book_tfidf
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**


```{r}
book_tfidf %>%
  arrange(-tf_idf)
```

**U N S C R A M B L E**

```{r}
book_tfidf %>%
  group_by(title) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(fct_reorder(word, tf_idf), tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~title, scales = "free")
```

## N-grams... and BEYOND

```{r}
tidy_ngram <- full_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

tidy_ngram
```

What are the most common bigrams?

```{r}
tidy_ngram %>%
  count(bigram, sort = TRUE)
```

Let's use `separate()` from tidyr to remove stop words.

```{r}
bigram_counts <- tidy_ngram %>%
  separate(bigram, into = c("word1","word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigram_counts
```

## Network analysis

Create a word network from bigrams!

```{r}
library(widyr)
library(igraph)
library(ggraph)

bigram_graph <- bigram_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame()

bigram_graph
```

Visualize the network.

```{r}
bigram_graph %>%
  ggraph(layout = "nicely") +
  geom_edge_link(aes(edge_alpha = n)) + 
  geom_node_text(aes(label = name)) +    
  theme_graph() 
```


Lots of ways to make the graph nicer!

```{r}
bigram_graph %>%
  ggraph(layout = "nicely") +
  geom_edge_link(aes(edge_alpha = n),
                 show.legend = FALSE, 
                 arrow = arrow(length = unit(1.5, 'mm')), 
                 start_cap = circle(3, 'mm'),
                 end_cap = circle(3, 'mm')) + 
  geom_node_text(aes(label = word)) +    
  theme_graph() 
```

See you tomorrow! <3
