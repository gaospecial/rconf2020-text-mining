---
title: "Text modeling -- rstudio::conf 2020"
date: "2020/01/28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Welcome to day #2!

## Topic modeling

First download data to use in modeling:

https://www.gutenberg.org/browse/scores/top

Replace one to four of the books below with your own choice(s).

```{r}
library(tidyverse)
library(gutenbergr)

titles <- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title", 
                     mirror = "https://www.gutenberg.org")
```

> 古登堡计划（Project Gutenberg），由志愿者参与，致力于将文化作品的数字化和归档，
并鼓励创作和发行电子书。该工程肇始于1971年，是最早的数字图书馆。
其中的大部分书籍都是公有领域书籍的原本，谷登堡计划确保这些原本自由流通、
自由文件格式，有利于长期保存，并可在各种计算机上阅读。
截至2018年7月，谷登堡计划声称超过57,000件馆藏。

`gutenbergr` 软件包包含了访问该项目的一些 R 语言工具。
例如 `gutenberg_download()` 可以用来下载电子书。
在使用过程中，发现需要设置 `mirror` 参数才可以成功（软件内的 `r gutenberg_get_mirror()` 可能已经失效）。

```{r}
books %>%
  count(title)
```



What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

电子书的每一行只有一句话或几个单词，下面的代码以书名 + 章节对每句话进行区分。

```{r}
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, 
                                     regex("^chapter ", 
                                           ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

by_chapter
```

Someone has TORN YOUR BOOKS APART!!!

## Let's use topic modeling to put your books back together

As a first step, let's tokenize and tidy these chapters.

```{r}
library(tidytext)

word_counts <- by_chapter %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  count(document, word, sort = TRUE)

word_counts
```

Next, let's **cast** to a sparse matrix. 

How many features and observations do you have?

```{r}
words_sparse <- word_counts %>%
  cast_sparse(document, word, n)

class(words_sparse)
dim(words_sparse)
```

Train a topic model.

```{r}
library(stm)

topic_model <- stm(words_sparse, K = 4, 
                   init.type = "Spectral")

summary(topic_model)
```

## Explore the output of topic modeling

The word-topic probabilities are called the "beta" matrix.

```{r}
chapter_topics <- tidy(topic_model, matrix = "beta")

# beta 的值保存在 topic_model$beta$logbeta[[1]] 中
chapter_topics %>% mutate(log_beta = log(beta))
topic_model$beta$logbeta[[1]][,1:5]
```

What are the highest probability words in each topic?

**U N S C R A M B L E**

```{r}
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

```

Let's build a visualization.

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta,topic)) %>%
  ggplot(aes(term,beta,fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_x_reordered() +
  coord_flip() +
  labs(y=expression(beta))
```

The document-topic probabilities are called "gamma".

```{r}
chapters_gamma <- tidy(topic_model, matrix = "gamma",
                       document_names = rownames(words_sparse))

glimpse(chapters_gamma)
```

How well did we do in putting our books back together into the 4 topics?

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
chapters_parsed <- chapters_gamma %>%
  separate(document, c("title", "chapter"), 
           sep = "_", convert = TRUE)

glimpse(chapters_parsed)
```

Let's visualize the results.

**U N S C R A M B L E**

```{r}
chapters_parsed %>%
  mutate(title = fct_reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title, ncol = 2)
```

Train many topic models to find the "right" value for K.

```{r}
many_models <- tibble(K = c(3,4,6,8,10)) %>%
  mutate(topic_model = map(K, 
                                  ~stm(words_sparse, K = .,
                                       verbose = FALSE)))
many_models
```

Evaluate metrics for these topic models. Lots to work through!

```{r}
heldout <- make.heldout(words_sparse)

k_result <- many_models %>%
  mutate(exclusivity        = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, words_sparse),
         eval_heldout       = map(topic_model, eval.heldout, heldout$missing),
         residual           = map(topic_model, checkResiduals, words_sparse),
         bound              = map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact              = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound             = bound + lfact,
         iterations         = map_dbl(topic_model, function(x) length(x$convergence$bound)))         
```

How do model metrics change with K?

```{r}
k_result %>%
  transmute(K,
            `Lower bound`         = lbound,
            Residuals             = map_dbl(residual, "dispersion"),
            `Semantic coherence`  = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL)
```

What is the relationship between semantic coherence and exclusivity?

```{r}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(3, 6, 10)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, 
             color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity")
```


## Text classification

Let's get two texts and build a model to distinguish between them.

Replace one or two of the books below with your own choice(s).

```{r}
titles <- c("The War of the Worlds",
            "Pride and Prejudice")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title") %>%
  mutate(document = row_number())

books
```

By making the `document` column and using that as our modeling unit, we are splitting each book up until its individual lines, as given to us by Project Gutenberg.

Next, let's make a tidy, tokenized dataset.

```{r}
tidy_books <- books %>%
  unnest_tokens(word, text) %>%
  group_by(word) %>%
  filter(n() > 10) %>%
  ungroup

tidy_books
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
library(rsample)

books_split <- tidy_books %>%
  distinct(document) %>%
  initial_split()

train_data <- training(books_split)
test_data <- testing(books_split)
```

Next build:

- a sparse matrix with the features to use in modeling
- a dataframe with the **response** variable (i.e. title)

```{r}
sparse_words <- tidy_books %>%
  count(document, word, sort = TRUE) %>%
  inner_join(train_data) %>%
  cast_sparse(document, word, n)

class(sparse_words)
dim(sparse_words)
```

## Build a dataframe with the response variable

```{r}
word_rownames <- as.integer(rownames(sparse_words))

books_joined <- tibble(document = word_rownames) %>%
  left_join(books %>%
              select(document, title))

books_joined
```


## Train a regularized regression model

```{r}
library(glmnet)
# library(doMC)            # 在 Unix 平台使用多个核心
# registerDoMC(cores = 8)

is_jane <- books_joined$title == "Pride and Prejudice"
model <- cv.glmnet(sparse_words, is_jane, 
                   family = "binomial", 
                   parallel = TRUE,
                   keep = TRUE)
```

You can also check out the built-in `plot(model)` results from glmnet.

## Understand and evaluate the model

How does the glmnet model classify each document?

```{r}
library(broom)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)

Intercept <- coefs %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
```

**U N S C R A M B L E**

```{r}
classifications <- tidy_books %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(document) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(Intercept + score))
```

What are the coefficients? Which ones contribute the most?

**U N S C R A M B L E**

```{r}
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup %>%
  ggplot(aes(fct_reorder(term, estimate), 
             estimate, 
             fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip()
```

What do you predict will happen if we run the following code?

**PREDICT WITH YOUR NEIGHBOR BEFORE YOU RUN**

```{r}
comment_classes <- classifications %>%
  left_join(books %>%
              select(title, document), by = "document") %>%
  mutate(title = as.factor(title))

comment_classes
```

Let's build an ROC ([receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) curve.

```{r}
library(yardstick)

comment_classes %>%
  roc_curve(title, probability) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```

What is the AUC?

```{r}
comment_classes %>%
  roc_auc(title, probability)
```

What about a confusion matrix?

```{r}
comment_classes %>%
  mutate(
    prediction = case_when(
      probability > 0.5 ~ "Pride and Prejudice",
      TRUE ~ "The War of the Worlds"
    ),
    prediction = as.factor(prediction)
  ) %>%
  conf_mat(title, prediction)
```

Now let's talk about misclassifications.

```{r}
## 分类器认为是但实际不是的句子（假阳性，FP）
comment_classes %>%
  filter(
    probability > 0.9,
    title == "The War of the Worlds"
  ) %>%
  sample_n(5) %>%
  inner_join(books %>%
               select(document, text)) %>%
  select(probability, text)
```

```{r}
# 分类器认为不是但是是的句子（假阴性，FN）
comment_classes %>%
  filter(
    probability < 0.2,
    title != "The War of the Worlds"
  ) %>%
  sample_n(5) %>%
  inner_join(books %>%
               select(document, text)) %>%
  select(probability, text)
```

How should you change this code to see the **other** kind of misclassification?

**GO EXPLORE REAL-WORLD TEXT!**

Thanks for coming! <3
